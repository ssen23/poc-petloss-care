"""
ëŠ˜í’ˆ RAG ê²€ìƒ‰ ì—”ì§„ v2.1
Multi-Intent Support + Flexible Parsing
"""

import json
import os
from typing import List, Dict, Any, Optional
import re

class NeulPoomRAG:
    """
    ëŠ˜í’ˆ RAG ê²€ìƒ‰ ì—”ì§„ v2.1
    - ë³µí•© Intent ì§€ì› (í•œ ì§ˆë¬¸ì— ì—¬ëŸ¬ ì˜ë„)
    - ì£„ì±…ê° ê°ì§€ ì‹œ ìžë™ìœ¼ë¡œ medical_comfort_facts ê²€ìƒ‰
    """
    
    def __init__(self, data_dir="./data"):
        self.data_dir = data_dir
        self.core_logic = {}
        self.exoneration_facts = {}  # âœ¨ 02_domain_knowledge â†’ 02_exoneration_facts
        self.structured_data = {}
        
        # ë°ì´í„° ë¡œë”©
        self._load_all_data()
    
    def _load_all_data(self):
        """ëª¨ë“  ë°ì´í„° ë¡œë”©"""
        print("ðŸ”„ RAG ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # 1. Core Logic (ìƒë‹´ í”„ë¡œí† ì½œ, CBT, Continuing Bonds)
        core_path = os.path.join(self.data_dir, "01_core_logic")
        if os.path.exists(core_path):
            self.core_logic = self._load_markdown(core_path)
            print(f"âœ… Core Logic: {len(self.core_logic)}ê°œ íŒŒì¼")
        
        # 2. Exoneration Facts (ì£„ì±…ê° í•´ì†Œ íŒ©íŠ¸)
        exoneration_path = os.path.join(self.data_dir, "02_exoneration_facts")
        if os.path.exists(exoneration_path):
            self.exoneration_facts = self._load_markdown(exoneration_path)
            print(f"âœ… Exoneration Facts: {len(self.exoneration_facts)}ê°œ íŒŒì¼")
        else:
            # í•˜ìœ„ í˜¸í™˜: 02_domain_knowledgeë„ ì‹œë„
            domain_path = os.path.join(self.data_dir, "02_domain_knowledge")
            if os.path.exists(domain_path):
                self.exoneration_facts = self._load_markdown(domain_path)
                print(f"âš ï¸ Domain Knowledge (legacy): {len(self.exoneration_facts)}ê°œ íŒŒì¼")
        
        # 3. Structured Data (JSON)
        structured_path = os.path.join(self.data_dir, "03_structured_data")
        if os.path.exists(structured_path):
            self.structured_data = self._load_json(structured_path)
            print(f"âœ… Structured Data: {len(self.structured_data)}ê°œ íŒŒì¼")
        
        print("âœ… RAG ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    
    def _load_markdown(self, path: str) -> Dict[str, str]:
        """Markdown íŒŒì¼ ë¡œë”©"""
        files = {}
        if os.path.exists(path):
            for filename in os.listdir(path):
                if filename.endswith('.md'):
                    filepath = os.path.join(path, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        files[filename] = f.read()
        return files
    
    def _load_json(self, path: str) -> Dict[str, Any]:
        """JSON íŒŒì¼ ë¡œë”©"""
        files = {}
        if os.path.exists(path):
            for filename in os.listdir(path):
                if filename.endswith('.json'):
                    filepath = os.path.join(path, filename)
                    with open(filepath, 'r', encoding='utf-8') as f:
                        files[filename] = json.load(f)
        return files
    
    def search(
        self, 
        query: str, 
        intent: str = "auto",
        max_results: int = 3
    ) -> Dict[str, Any]:
        """
        í†µí•© ê²€ìƒ‰ (ë³µí•© Intent ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            intent: "emotional", "factual", "service", "guilt", "auto"
            max_results: ê° ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            {
                "core_logic": [...],
                "exoneration_facts": [...],
                "structured_data": [...],
                "detected_intents": [...]  # âœ¨ ë³µìˆ˜í˜•!
            }
        """
        # Intent ìžë™ ê°ì§€ (ë³µìˆ˜ ë°˜í™˜)
        if intent == "auto":
            intents = self._detect_intents(query)  # âœ¨ ë³µìˆ˜í˜• í•¨ìˆ˜
        else:
            intents = [intent]
        
        results = {
            "core_logic": [],
            "exoneration_facts": [],
            "structured_data": [],
            "detected_intents": intents  # âœ¨ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ìž¥
        }
        
        # 1. ê°ì •/ìƒë‹´ ê´€ë ¨ â†’ Core Logic ê²€ìƒ‰
        if "emotional" in intents or "auto" in intents:
            results["core_logic"] = self._search_core_logic(query, max_results)
        
        # 2. ì£„ì±…ê° ê´€ë ¨ â†’ Exoneration Facts ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        if "guilt" in intents:
            results["exoneration_facts"] = self._search_exoneration_facts(query, max_results)
        
        # 3. ì •ë³´/ì§€ì‹ ê´€ë ¨ â†’ Exoneration Facts ê²€ìƒ‰
        if "factual" in intents:
            # factualì´ì§€ë§Œ ì£„ì±…ê° í‚¤ì›Œë“œê°€ ìžˆìœ¼ë©´ exoneration_factsë„ ê²€ìƒ‰
            if not results["exoneration_facts"]:  # ì•„ì§ ì•ˆ ì°¾ì•˜ìœ¼ë©´
                results["exoneration_facts"] = self._search_exoneration_facts(query, max_results)
        
        # 4. ì„œë¹„ìŠ¤/ì—…ì²´ ê´€ë ¨ â†’ Structured Data ê²€ìƒ‰
        if "service" in intents or "auto" in intents:
            results["structured_data"] = self._search_structured_data(query, max_results)
        
        return results
    
    def _detect_intents(self, query: str) -> List[str]:
        """
        ì§ˆë¬¸ ì˜ë„ íŒŒì•… (ë³µí•© Intent ì§€ì›)
        
        Returns:
            List[str]: ê°ì§€ëœ ëª¨ë“  ì˜ë„ë“¤
        """
        intents = []
        query_lower = query.lower()
        
        # ìš°ì„ ìˆœìœ„ 1: ì£„ì±…ê° (ê°€ìž¥ ì¤‘ìš”!)
        guilt_keywords = [
            "ë¯¸ì•ˆ", "ì£„ì±…ê°", "ë‚´ íƒ“", "ìž˜ëª»", "í›„íšŒ",
            "ëŠ¦ê²Œ", "ë†“ì³¤", "ì•Œì•˜ì–´ì•¼", "ì¡°ê¸‰í–ˆë‚˜",
            "ë„ˆë¬´ ì¼ì°", "ë„ˆë¬´ ëŠ¦ê²Œ", "ì‚´ë ¸ì„", "ì£½ì¸"
        ]
        if any(k in query_lower for k in guilt_keywords):
            intents.append("guilt")
        
        # ìš°ì„ ìˆœìœ„ 2: ì„œë¹„ìŠ¤ (ëª…í™•í•œ ìš”ì²­)
        service_keywords = [
            "ìž¥ë¡€", "ì‹ìž¥", "ìƒë‹´", "ì¶”ì²œ", "ì–´ë””", "ì „í™”", "ì˜ˆì•½", "ê°€ê²©",
            "ë„ì›€", "ë°›ê³  ì‹¶", "ì—°ê²°", "ì°¾ì•„", "í•«ë¼ì¸", "ì„¼í„°",
            "ìƒë‹´ì‚¬", "ì „ë¬¸ê°€", "ë³‘ì›", "ì¹˜ë£Œ", "ë¬¸ì˜", "ì•Œë ¤"
        ]
        if any(k in query_lower for k in service_keywords):
            intents.append("service")
        
        # ìš°ì„ ìˆœìœ„ 3: ì •ë³´ (ì§ˆë³‘, ë²•ë¥ )
        factual_keywords = [
            "ì¦ìƒ", "ì§ˆë³‘", "ì‹ ë¶€ì „", "ì‹¬ìž¥", "ì‹¬ìž¥ë³‘", "ì•”",
            "ë²•ë¥ ", "ë“±ë¡", "ë§ì†Œ", "ë³´í—˜",
            "ê²½ë ¨", "í˜¸í¡", "ìš”ë…", "ë°œìž‘", "í–‰ì •", "ì‹ ê³ "
        ]
        if any(k in query_lower for k in factual_keywords):
            intents.append("factual")
        
        # ìš°ì„ ìˆœìœ„ 4: ê°ì • (ê¸°ë³¸ê°’)
        emotional_keywords = [
            "ìŠ¬í”„", "ì£„ì±…ê°", "íž˜ë“¤", "ê´´ë¡­", "ìš°ìš¸", "ë¯¸ì•ˆ", "ë³´ê³  ì‹¶",
            "ê·¸ë¦¬", "ì™¸ë¡œ", "ì•„í”„", "ë¬´ë„ˆ", "ëˆˆë¬¼", "ìš¸", "ê²¬ë””",
            "íž˜ë“¤ì–´", "ê´´ë¡œì›Œ", "ìŠ¬í¼", "ì•„íŒŒ"
        ]
        if any(k in query_lower for k in emotional_keywords):
            intents.append("emotional")
        
        # ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ emotional (ê³µê°ì´ ë² ì´ìŠ¤)
        if not intents:
            intents.append("emotional")
        
        # ì¤‘ë³µ ì œê±°
        return list(set(intents))
    
    def _search_core_logic(self, query: str, max_results: int) -> List[Dict]:
        """
        Core Logic ê²€ìƒ‰ (ìƒë‹´ í”„ë¡œí† ì½œ, CBT, Continuing Bonds)
        í‚¤ì›Œë“œ ë§¤ì¹­ + ìœ ì—°í•œ íŒŒì‹±
        """
        matches = []
        query_lower = query.lower()
        
        for filename, content in self.core_logic.items():
            # CHUNK ë‹¨ìœ„ë¡œ ë¶„í•  (ìœ ì—°í•œ íŒŒì‹±)
            # ### [CHUNK, ###[CHUNK, ## [CHUNK ëª¨ë‘ í—ˆìš©
            chunks = re.split(r'###+\s*\[?CHUNK', content, flags=re.IGNORECASE)
            
            for i, chunk in enumerate(chunks[1:], start=1):  # CHUNK 1ë¶€í„°
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata_match = re.search(r'\*\*Metadata:\*\*\n(.*?)\n\n', chunk, re.DOTALL)
                keywords = []
                if metadata_match:
                    metadata_text = metadata_match.group(1)
                    keyword_match = re.search(r'Keywords?:\s*(.+)', metadata_text, re.IGNORECASE)
                    if keyword_match:
                        keywords = [k.strip() for k in keyword_match.group(1).split(',')]
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                score = 0
                for keyword in keywords:
                    if keyword.lower() in query_lower:
                        score += 2  # í‚¤ì›Œë“œ ë§¤ì¹­ì€ ë†’ì€ ì ìˆ˜
                
                # ë³¸ë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­
                chunk_words = query_lower.split()
                for word in chunk_words:
                    if len(word) > 1 and word in chunk.lower():
                        score += 0.5
                
                if score > 0:
                    matches.append({
                        "source": filename,
                        "chunk_id": f"CHUNK {i}",
                        "content": chunk[:800],  # 800ìžë§Œ
                        "score": score,
                        "keywords": keywords
                    })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        matches.sort(key=lambda x: x["score"], reverse=True)
        return matches[:max_results]
    
    def _search_exoneration_facts(self, query: str, max_results: int) -> List[Dict]:
        """
        Exoneration Facts ê²€ìƒ‰ (ì£„ì±…ê° í•´ì†Œ íŒ©íŠ¸)
        Scenario ë‹¨ìœ„ ê²€ìƒ‰
        """
        matches = []
        query_lower = query.lower()
        
        for filename, content in self.exoneration_facts.items():
            # SCENARIO ë‹¨ìœ„ë¡œ ë¶„í•  (ìœ ì—°í•œ íŒŒì‹±)
            scenarios = re.split(r'##\s*\[?SCENARIO', content, flags=re.IGNORECASE)
            
            for i, scenario in enumerate(scenarios[1:], start=1):
                # ì œëª© ì¶”ì¶œ
                title_match = re.search(r'^\s*\d+\]?\s*"?(.+?)"?\s*$', scenario.split('\n')[0])
                title = title_match.group(1) if title_match else f"Scenario {i}"
                
                # Keywords ì¶”ì¶œ
                keywords_match = re.search(r'\*\*Keywords\*\*:\s*(.+)', scenario, re.IGNORECASE)
                keywords = []
                if keywords_match:
                    keywords = [k.strip() for k in keywords_match.group(1).split(',')]
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                score = 0
                for keyword in keywords:
                    if keyword.lower() in query_lower:
                        score += 3  # Exonerationì€ ë” ë†’ì€ ì ìˆ˜
                
                # ì œëª© ë§¤ì¹­
                if any(word in title.lower() for word in query_lower.split()):
                    score += 2
                
                if score > 0:
                    matches.append({
                        "source": filename,
                        "scenario": title,
                        "content": scenario[:600],  # 600ìžë§Œ
                        "score": score,
                        "keywords": keywords
                    })
        
        matches.sort(key=lambda x: x["score"], reverse=True)
        return matches[:max_results]
    
    def _search_structured_data(self, query: str, max_results: int) -> List[Dict]:
        """
        Structured Data ê²€ìƒ‰ (JSON)
        - emergency_hotlines.json
        - peer_stories.json (âœ¨ ì‹ ê·œ)
        - funeral_homes.json
        """
        results = []
        query_lower = query.lower()
        
        # 1. ìœ„ê¸° í•«ë¼ì¸ ê²€ìƒ‰
        hotline_keywords = [
            "ìƒë‹´", "ì „í™”", "ë„ì›€", "í•«ë¼ì¸", "ì„¼í„°",
            "íž˜ë“¤", "ê´´ë¡­", "ìš°ìš¸", "ì£½ê³ ", "ìžì‚´",
            "ë°›ê³  ì‹¶", "ì—°ê²°", "ì°¾ì•„", "ë¬¸ì˜", "ì•Œë ¤"
        ]
        
        if any(k in query_lower for k in hotline_keywords) and "emergency_hotlines.json" in self.structured_data:
            hotlines = self.structured_data["emergency_hotlines.json"]
            
            matched = []
            for hotline in hotlines:
                score = 0
                for keyword in hotline.get("keywords", []):
                    if keyword in query_lower:
                        score += 2
                
                if score > 0 or len(matched) < 3:  # ìµœì†Œ 3ê°œ
                    matched.append((hotline, score))
            
            matched.sort(key=lambda x: x[1], reverse=True)
            results.extend([{
                "type": "hotline",
                "data": h[0]
            } for h in matched[:max_results]])
        
        # 2. âœ¨ í”¼ì–´ ìŠ¤í† ë¦¬ ê²€ìƒ‰ (ì‹ ê·œ)
        story_keywords = [
            "ì‚¬ë¡€", "ë‹¤ë¥¸ ë¶„", "ê²½í—˜", "ì´ì•¼ê¸°", "í›„ê¸°",
            "ì–´ë–»ê²Œ", "ê·¹ë³µ", "íšŒë³µ", "ë‚˜ì•„", "ê´œì°®"
        ]
        
        if any(k in query_lower for k in story_keywords) and "peer_stories.json" in self.structured_data:
            stories = self.structured_data["peer_stories.json"]
            
            matched_stories = []
            for story in stories:
                score = 0
                # ìƒí™© ë§¤ì¹­
                situation = story.get("situation", "")
                for word in query_lower.split():
                    if len(word) > 1 and word in situation.lower():
                        score += 1
                
                # ê°ì • ë§¤ì¹­
                emotion = story.get("emotion", "")
                for word in query_lower.split():
                    if len(word) > 1 and word in emotion.lower():
                        score += 2
                
                if score > 0:
                    matched_stories.append((story, score))
            
            matched_stories.sort(key=lambda x: x[1], reverse=True)
            results.extend([{
                "type": "peer_story",
                "data": s[0]
            } for s in matched_stories[:max_results]])
        
        # 3. ìž¥ë¡€ì‹ìž¥ ê²€ìƒ‰
        if ("ìž¥ë¡€" in query_lower or "í™”ìž¥" in query_lower) and "funeral_homes.json" in self.structured_data:
            funeral_homes = self.structured_data["funeral_homes.json"]
            
            # ì§€ì—­ í•„í„°ë§
            region = None
            if "ì„œìš¸" in query_lower: region = "ì„œìš¸"
            elif "ê²½ê¸°" in query_lower: region = "ê²½ê¸°"
            elif "ì¸ì²œ" in query_lower: region = "ì¸ì²œ"
            
            filtered = funeral_homes
            if region:
                filtered = [f for f in filtered if f.get("region") == region]
            
            filtered.sort(key=lambda x: x.get("rating", 0), reverse=True)
            results.extend([{
                "type": "funeral_home",
                "data": f
            } for f in filtered[:max_results]])
        
        return results
    
    def get_context_for_llm(self, search_results: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì´ ì½ì„ ìˆ˜ ìžˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        context_parts = []
        
        # 1. Exoneration Facts (ì£„ì±…ê° í•´ì†Œ - ìµœìš°ì„ )
        if search_results["exoneration_facts"]:
            context_parts.append("[ì£„ì±…ê° í•´ì†Œë¥¼ ìœ„í•œ íŒ©íŠ¸]")
            context_parts.append("âš ï¸ ì£¼ì˜: ì´ ì •ë³´ëŠ” ë³´í˜¸ìžì˜ ì£„ì±…ê°ì„ ëœì–´ì£¼ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.")
            context_parts.append("ì ˆëŒ€ ì§ˆë³‘ì„ ì„¤ëª…í•˜ë ¤ ë“¤ì§€ ë§ˆì„¸ìš”.\n")
            for item in search_results["exoneration_facts"]:
                context_parts.append(f"\n[{item['scenario']}]:")
                context_parts.append(item['content'])
        
        # 2. Core Logic (ìƒë‹´ í”„ë¡œí† ì½œ)
        if search_results["core_logic"]:
            context_parts.append("\n\n[ìƒë‹´ ê°€ì´ë“œë¼ì¸]")
            for item in search_results["core_logic"]:
                context_parts.append(f"\n{item['chunk_id']}:")
                context_parts.append(item['content'])
        
        # 3. Structured Data
        if search_results["structured_data"]:
            context_parts.append("\n\n[ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´]")
            for item in search_results["structured_data"]:
                if item["type"] == "hotline":
                    data = item["data"]
                    context_parts.append(f"\n- {data['name']}: {data['number']}")
                    context_parts.append(f"  ìš´ì˜: {data['available']}")
                    context_parts.append(f"  ì„¤ëª…: {data['description']}")
                
                elif item["type"] == "peer_story":
                    data = item["data"]
                    context_parts.append(f"\n[ë¹„ìŠ·í•œ ì‚¬ë¡€]")
                    context_parts.append(f"ìƒí™©: {data['situation']}")
                    context_parts.append(f"ê°ì •: {data['emotion']}")
                    context_parts.append(f"ê·¹ë³µ: {data['healing_journey']}")
                
                elif item["type"] == "funeral_home":
                    data = item["data"]
                    context_parts.append(f"\n- {data['name']}")
                    context_parts.append(f"  ìœ„ì¹˜: {data['address']}")
                    context_parts.append(f"  ì „í™”: {data['tel']}")
        
        return "\n".join(context_parts)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    rag = NeulPoomRAG(data_dir="./data")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ëŠ¦ê²Œ ë³‘ì›ì— ê°€ì„œ ë¯¸ì•ˆí•´ìš”",  # guilt + emotional
        "ì‹ ë¶€ì „ìœ¼ë¡œ ë– ë‚¬ëŠ”ë° ë„ˆë¬´ ìŠ¬í¼ìš”",  # factual + emotional
        "ì „ë¬¸ ìƒë‹´ ë°›ê³  ì‹¶ì–´ìš”"  # service
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"Query: {query}")
        print(f"{'='*60}")
        
        results = rag.search(query)
        print(f"Detected Intents: {results['detected_intents']}")  # âœ¨ ë³µìˆ˜í˜•!
        print(f"Core Logic: {len(results['core_logic'])} results")
        print(f"Exoneration Facts: {len(results['exoneration_facts'])} results")
        print(f"Structured Data: {len(results['structured_data'])} results")
        
        context = rag.get_context_for_llm(results)
        print(f"\n[LLM Context Preview]")
        print(context[:500] + "..." if len(context) > 500 else context)
